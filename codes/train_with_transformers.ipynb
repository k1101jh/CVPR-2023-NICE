{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from textwrap import wrap\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataset_config import COCO_dataset_config\n",
    "from utils import get_device_map\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = [1, 5, 6, 7]\n",
    "start_device = 'cuda:' + str(devices[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train result save dir name\n",
    "results_dir = '../results'\n",
    "result_dirname = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "result_dir_fullpath = os.path.join(results_dir, result_dirname)\n",
    "os.makedirs(result_dir_fullpath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%I:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(result_dir_fullpath, 'train.log')),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "writer = SummaryWriter(os.path.join('../runs', result_dirname))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blip2 설정 참고하기\n",
    "# https://github.com/salesforce/LAVIS/blob/main/lavis/projects/blip2/train/caption_coco_ft.yaml\n",
    "\n",
    "checkpoint = \"Salesforce/blip2-flan-t5-xl\"\n",
    "# cache_dir = \"/mnt/nas2/kjh/huggingface_cache\"\n",
    "cache_dir = \"../pretrained_files\"\n",
    "cfg_path = \"../configs/caption_coco_ft.yaml\"\n",
    "dtype = torch.float16\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "max_length = 50\n",
    "epochs = 5\n",
    "prompt = \"a photo of \"\n",
    "\n",
    "model_name = checkpoint.split(\"/\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "config = OmegaConf.load(cfg_path)\n",
    "\n",
    "print(config)\n",
    "\n",
    "def dict_to_str_recursive(input_dict, depth=0):\n",
    "    result_str = ''\n",
    "    indent_str = '&nbsp;&nbsp;&nbsp;&nbsp;' * depth\n",
    "    for key in input_dict:\n",
    "        if type(input_dict[key]) in [dict, DictConfig]:\n",
    "            value_str = dict_to_str_recursive(input_dict[key], depth + 1)\n",
    "            result_str += indent_str + str(key) + ':  \\n' + value_str + '  \\n'\n",
    "        else:\n",
    "            value_str = str(input_dict[key])\n",
    "            result_str += indent_str + str(key) + ': ' + value_str + '  \\n'\n",
    "    return result_str\n",
    "\n",
    "config_str = dict_to_str_recursive(config)\n",
    "writer.add_text('configs', config_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor\n",
    "##### image-processor + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\n",
    "    checkpoint,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train_ds = load_dataset('../datasets/cvpr-nice-val/', split='validation')\n",
    "caption_ds = load_dataset('../datasets/cvpr-nice-val', data_files={'caption': 'nice-val-5k.csv'}, split='caption')\n",
    "for feature in caption_ds.features:\n",
    "    print(feature)\n",
    "    train_ds = train_ds.add_column(name=feature, column=caption_ds[feature])\n",
    "    \n",
    "# column명 변경\n",
    "train_ds.rename_column(\"image\", \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens = processor.tokenizer(\n",
    "    prompt, padding='max_length', max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(input_batch, prefix=None):\n",
    "    if prefix is not None:\n",
    "        input_batch['caption_gt'] = prefix + input_batch['caption_gt']\n",
    "    # batch = processor(images=batch['image'], text=batch['caption_gt'], padding=\"max_length\", max_length=max_length, return_tensors='pt')\n",
    "    batch = processor(images=input_batch['image'], text=prompt, padding=\"max_length\", max_length=max_length, return_tensors='pt')\n",
    "    batch['pixel_values'] = batch['pixel_values'].squeeze(0)\n",
    "    batch.update({'labels': processor.tokenizer(input_batch['caption_gt'], padding='max_length', max_length=max_length)})\n",
    "    # batch.update({'decoder_input_ids': prompt_tokens['input_ids']})\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(\n",
    "    transforms,\n",
    "    remove_columns=['public_id', 'caption_gt', 'image', 'category'],\n",
    ")\n",
    "# batch 설정하면 왜인지 pixel_values가 이상해짐 & 변환할 때 image가 있어서 그런지 더 느려짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(normalized_image, mean, std):\n",
    "    image = normalized_image.transpose(1, 2, 0)\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def plot_images(images, captions):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images)):\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        caption = captions[i]\n",
    "        caption = \"\\n\".join(wrap(caption, 16))\n",
    "        plt.title(caption)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "num_samples = 5\n",
    "samples = [train_ds[i] for i in range(5)]\n",
    "\n",
    "sample_images = []\n",
    "sample_captions = []\n",
    "for i in range(num_samples):\n",
    "    sample_image = np.array(samples[i]['pixel_values'])\n",
    "    sample_image = denormalize_image(sample_image, processor.image_processor.image_mean, processor.image_processor.image_std)\n",
    "    sample_images.append(sample_image)\n",
    "    \n",
    "    sample_caption = ' '.join(processor.batch_decode(samples[i]['labels'], skip_special_tokens=True))\n",
    "    sample_captions.append(sample_caption)\n",
    "\n",
    "plot_images(sample_images, sample_captions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2ForConditionalGeneration\n",
    "\n",
    "device_map = get_device_map(checkpoint, devices)\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    checkpoint,\n",
    "    cache_dir=cache_dir,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=device_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze\n",
    "block_list = [\n",
    "    model.vision_model,\n",
    "    model.qformer,\n",
    "    model.language_projection,\n",
    "    model.language_model,\n",
    "]\n",
    "\n",
    "freeze_list = [\n",
    "    # model.vision_model,\n",
    "    # model.qformer,\n",
    "    # model.language_projection,\n",
    "    model.language_model,\n",
    "]\n",
    "\n",
    "for freeze_block in freeze_list:\n",
    "    for name, param in freeze_block.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    freeze_block = freeze_block.eval()\n",
    "\n",
    "for block in block_list:\n",
    "    if block not in freeze_list:\n",
    "        for name, param in block.named_parameters():\n",
    "            param.requires_grad = True\n",
    "    block = block.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../training_outputs/{model_name}\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=epochs,\n",
    "    fp16=True if dtype is torch.float16 else False,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=5,\n",
    "    logging_steps=1,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=['input_ids'],\n",
    ")\n",
    "training_args.set_lr_scheduler(name='linear', warmup_steps=1000)\n",
    "training_args.set_optimizer(name='adamw_hf', learning_rate=1e-6, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(output):\n",
    "    logits = output.loss.get('logits')\n",
    "    input_ids = output.loss.get('input_ids')\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = input_ids[..., 1:].contiguous().to(logits.device)\n",
    "    \n",
    "    print(torch.isnan(logits).any())\n",
    "    print(torch.isnan(shift_labels).any())\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fn(shift_logits.view(-1, 32128), shift_labels.view(-1))\n",
    "    return {\n",
    "        'loss': loss.item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=None,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_NICE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
