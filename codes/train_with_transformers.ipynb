{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils import get_device_map\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = [1, 5, 6, 7]\n",
    "start_device = 'cuda:' + str(devices[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train result save dir name\n",
    "results_dir = '../results'\n",
    "result_dirname = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "result_dir_fullpath = os.path.join(results_dir, result_dirname)\n",
    "os.makedirs(result_dir_fullpath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%I:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(result_dir_fullpath, 'train.log')),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blip2 설정 참고하기\n",
    "# https://github.com/salesforce/LAVIS/blob/main/lavis/projects/blip2/train/caption_coco_ft.yaml\n",
    "\n",
    "# checkpoint = \"Salesforce/blip2-opt-2.7b\"\n",
    "checkpoint = \"Salesforce/blip2-flan-t5-xl\"\n",
    "# cache_dir = \"/mnt/nas2/kjh/huggingface_cache\"\n",
    "cache_dir = \"../caches\"\n",
    "cache_pretrained_files_dir = os.path.join(cache_dir, \"pretrained_files\")\n",
    "cache_dataset_dir = os.path.join(cache_dir, \"datasets\")\n",
    "\n",
    "dtype = torch.float32       # 논문에는 float16이지만 이렇게 하면 loss가 nan이 되는 문제가 있음\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "max_length = 50\n",
    "epochs = 2\n",
    "learning_rate = 1e-6\n",
    "prompt = \"a photo of \"\n",
    "\n",
    "model_name = checkpoint.split(\"/\")[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor\n",
    "##### image-processor + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\n",
    "    checkpoint,\n",
    "    cache_dir=cache_pretrained_files_dir,\n",
    "    torch_dtype=dtype\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset, Image\n",
    "\n",
    "caption_ds = load_dataset('../datasets/cvpr-nice-val', data_files={'caption': 'nice-val-5k.csv'}, split='caption', cache_dir=cache_dataset_dir)\n",
    "\n",
    "image_filename_list = caption_ds['public_id']\n",
    "image_path_list = [os.path.join('../datasets/cvpr-nice-val/val', str(image_filename) + '.jpg') for image_filename in image_filename_list]\n",
    "train_ds = Dataset.from_dict({'image': image_path_list}).cast_column(\"image\", Image())\n",
    "\n",
    "for feature in caption_ds.features:\n",
    "    train_ds = train_ds.add_column(name=feature, column=caption_ds[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.add_special_tokens({\"bos_token\": \"[DEC]\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens = processor.tokenizer(\n",
    "    prompt, padding='max_length', max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(input_batch, prefix=None):\n",
    "    if prefix is not None:\n",
    "        input_batch['caption_gt'] = prefix + input_batch['caption_gt']\n",
    "    batch = processor(images=input_batch['image'], text=input_batch['caption_gt'], padding=\"max_length\", max_length=max_length, return_tensors='pt')\n",
    "    batch['pixel_values'] = batch['pixel_values'].squeeze(0)\n",
    "    batch.update({'labels': processor.tokenizer(input_batch['caption_gt'], padding='max_length', max_length=max_length)['input_ids']})\n",
    "    batch.update({'input_ids': batch['input_ids'].squeeze(0)})\n",
    "    del batch['attention_mask']     # 이거 해야하나 말아야하나.. forward에서 굳이 필요 없는거 같은데...\n",
    "                                    # 일단 주석처리 하는 경우 훈련 과정에서 차원이 안맞는 문제 발생할 것\n",
    "    return batch\n",
    "\n",
    "# def transforms(input_batch, prefix=None):\n",
    "#     if prefix is not None:\n",
    "#         input_batch['caption_gt'] = prefix + input_batch['caption_gt']\n",
    "#     batch = processor(images=input_batch['image'], text=input_batch['caption_gt'], padding=\"max_length\", max_length=max_length, return_tensors='pt')\n",
    "#     batch['pixel_values'] = batch['pixel_values'].squeeze(0)\n",
    "#     # batch.update({'labels': processor.tokenizer(input_batch['caption_gt'], padding='max_length', max_length=max_length)['input_ids']})\n",
    "#     batch.update({'labels': batch['input_ids']})\n",
    "#     batch.update({'input_ids': batch['input_ids'].squeeze(0)})\n",
    "#     del batch['attention_mask']     # 이거 해야하나 말아야하나.. forward에서 굳이 필요 없는거 같은데...\n",
    "#                                     # 일단 주석처리 하는 경우 훈련 과정에서 차원이 안맞는 문제 발생할 것\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(\n",
    "    transforms,\n",
    "    remove_columns=['public_id', 'caption_gt', 'image', 'category'],\n",
    ")\n",
    "# batch 설정하면 왜인지 pixel_values가 이상해짐 & 변환할 때 조금 더 느려짐\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import denormalize_image, plot_images\n",
    "\n",
    "num_samples = 5\n",
    "samples = [train_ds[i] for i in range(num_samples)]\n",
    "\n",
    "sample_images = []\n",
    "sample_captions = []\n",
    "for i in range(num_samples):\n",
    "    sample_image = np.array(samples[i]['pixel_values'])\n",
    "    sample_image = denormalize_image(sample_image, processor.image_processor.image_mean, processor.image_processor.image_std)\n",
    "    sample_images.append(sample_image)\n",
    "    \n",
    "    sample_caption = ' '.join(processor.batch_decode(samples[i]['labels'], skip_special_tokens=True))\n",
    "    sample_captions.append(sample_caption)\n",
    "\n",
    "plot_images(sample_images, sample_captions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2ForConditionalGeneration\n",
    "\n",
    "device_map = get_device_map(checkpoint, devices)\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    checkpoint,\n",
    "    cache_dir=cache_pretrained_files_dir,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=device_map,\n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_list = [\n",
    "    model.vision_model,\n",
    "    model.qformer,\n",
    "    model.language_projection,\n",
    "    model.language_model,\n",
    "]\n",
    "\n",
    "# freeze 할 블록들\n",
    "freeze_list = [\n",
    "    # model.vision_model,\n",
    "    # model.qformer,\n",
    "    # model.language_projection,\n",
    "    model.language_model,\n",
    "]\n",
    "\n",
    "for freeze_block in freeze_list:\n",
    "    for name, param in freeze_block.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    freeze_block = freeze_block.eval()\n",
    "\n",
    "for block in block_list:\n",
    "    if block not in freeze_list:\n",
    "        for name, param in block.named_parameters():\n",
    "            param.requires_grad = True\n",
    "    block = block.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../training_outputs/{model_name}\",\n",
    "    do_train=True,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=epochs,\n",
    "    fp16=True if dtype is torch.float16 else False,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=1,\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    dataloader_drop_last=True,\n",
    "    logging_steps=100,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    # label_names=['labels'],\n",
    ")\n",
    "training_args.set_lr_scheduler(name='linear', warmup_steps=1000)\n",
    "training_args.set_optimizer(name='adamw_hf', learning_rate=learning_rate, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from utils import show_image_caption\n",
    "\n",
    "image = '../datasets/cvpr-nice-val/val/215268662.jpg'\n",
    "# caption_gt = 'Bicycles leaning against tree in wood Close up low angle view'\n",
    "raw_image = Image.open(image).convert('RGB')\n",
    "\n",
    "inputs = processor(raw_image, return_tensors=\"pt\").to(start_device, dtype)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=max_length)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n",
    "\n",
    "show_image_caption(raw_image, [generated_text], show_fig=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_NICE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
