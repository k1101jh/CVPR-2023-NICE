{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from textwrap import wrap\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataset_config import COCO_dataset_config\n",
    "from utils import get_device_map\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = [1, 5, 6, 7]\n",
    "start_device = 'cuda:' + str(devices[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train result save dir name\n",
    "results_dir = '../results'\n",
    "result_dirname = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "result_dir_fullpath = os.path.join(results_dir, result_dirname)\n",
    "os.makedirs(result_dir_fullpath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%I:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(result_dir_fullpath, 'train.log')),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "writer = SummaryWriter(os.path.join('../runs', result_dirname))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blip2 설정 참고하기\n",
    "# https://github.com/salesforce/LAVIS/blob/main/lavis/projects/blip2/train/caption_coco_ft.yaml\n",
    "\n",
    "# checkpoint = \"Salesforce/blip2-opt-2.7b\"\n",
    "checkpoint = \"Salesforce/blip2-flan-t5-xl\"\n",
    "# cache_dir = \"/mnt/nas2/kjh/huggingface_cache\"\n",
    "cache_dir = \"../caches\"\n",
    "cache_pretrained_files_dir = os.path.join(cache_dir, \"pretrained_files\")\n",
    "cache_dataset_dir = os.path.join(cache_dir, \"datasets\")\n",
    "cfg_path = \"../configs/caption_coco_ft.yaml\"\n",
    "dtype = torch.float32\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "max_length = 50\n",
    "epochs = 5\n",
    "learning_rate = 1e-4\n",
    "prompt = \"a photo of \"\n",
    "\n",
    "model_name = checkpoint.split(\"/\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "config = OmegaConf.load(cfg_path)\n",
    "\n",
    "print(config)\n",
    "\n",
    "def dict_to_str_recursive(input_dict, depth=0):\n",
    "    result_str = ''\n",
    "    indent_str = '&nbsp;&nbsp;&nbsp;&nbsp;' * depth\n",
    "    for key in input_dict:\n",
    "        if type(input_dict[key]) in [dict, DictConfig]:\n",
    "            value_str = dict_to_str_recursive(input_dict[key], depth + 1)\n",
    "            result_str += indent_str + str(key) + ':  \\n' + value_str + '  \\n'\n",
    "        else:\n",
    "            value_str = str(input_dict[key])\n",
    "            result_str += indent_str + str(key) + ': ' + value_str + '  \\n'\n",
    "    return result_str\n",
    "\n",
    "config_str = dict_to_str_recursive(config)\n",
    "writer.add_text('configs', config_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor\n",
    "##### image-processor + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\n",
    "    checkpoint,\n",
    "    cache_dir=cache_pretrained_files_dir,\n",
    "    torch_dtype=dtype\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset, Image\n",
    "\n",
    "caption_ds = load_dataset('../datasets/cvpr-nice-val', data_files={'caption': 'nice-val-5k.csv'}, split='caption', cache_dir=cache_dataset_dir)\n",
    "\n",
    "image_filename_list = caption_ds['public_id']\n",
    "image_path_list = [os.path.join('../datasets/cvpr-nice-val/val', str(image_filename) + '.jpg') for image_filename in image_filename_list]\n",
    "train_ds = Dataset.from_dict({'image': image_path_list}).cast_column(\"image\", Image())\n",
    "\n",
    "for feature in caption_ds.features:\n",
    "    train_ds = train_ds.add_column(name=feature, column=caption_ds[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens = processor.tokenizer(\n",
    "    prompt, padding='max_length', max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(input_batch, prefix=None):\n",
    "    if prefix is not None:\n",
    "        input_batch['caption_gt'] = prefix + input_batch['caption_gt']\n",
    "    batch = processor(images=input_batch['image'], text=prompt, padding=\"max_length\", max_length=max_length, return_tensors='pt')\n",
    "    batch['pixel_values'] = batch['pixel_values'].squeeze(0)\n",
    "    batch.update({'labels': processor.tokenizer(input_batch['caption_gt'], padding='max_length', max_length=max_length)['input_ids']})\n",
    "    batch.update({'input_ids': batch['input_ids'].squeeze(0)})\n",
    "    del batch['attention_mask']     # 이거 해야하나 말아야하나.. forward에서 'attention_mask' 지정 안해주면 알아서 만들어 주는 거 같은데\n",
    "    # batch.update({'decoder_input_ids': prompt_tokens['input_ids']})\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(\n",
    "    transforms,\n",
    "    remove_columns=['public_id', 'caption_gt', 'image', 'category'],\n",
    ")\n",
    "# batch 설정하면 왜인지 pixel_values가 이상해짐 & 변환할 때 조금 더 느려짐\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(normalized_image, mean, std):\n",
    "    image = normalized_image.transpose(1, 2, 0)\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def plot_images(images, captions, wrap_width=20):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images)):\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        caption = captions[i]\n",
    "        caption = \"\\n\".join(wrap(caption, wrap_width))\n",
    "        plt.title(caption)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "num_samples = 5\n",
    "samples = [train_ds[i] for i in range(num_samples)]\n",
    "\n",
    "sample_images = []\n",
    "sample_captions = []\n",
    "for i in range(num_samples):\n",
    "    sample_image = np.array(samples[i]['pixel_values'])\n",
    "    sample_image = denormalize_image(sample_image, processor.image_processor.image_mean, processor.image_processor.image_std)\n",
    "    sample_images.append(sample_image)\n",
    "    \n",
    "    sample_caption = ' '.join(processor.batch_decode(samples[i]['labels'], skip_special_tokens=True))\n",
    "    sample_captions.append(sample_caption)\n",
    "\n",
    "plot_images(sample_images, sample_captions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2ForConditionalGeneration\n",
    "\n",
    "device_map = get_device_map(checkpoint, devices)\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    checkpoint,\n",
    "    cache_dir=cache_pretrained_files_dir,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=device_map,\n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze\n",
    "block_list = [\n",
    "    model.vision_model,\n",
    "    model.qformer,\n",
    "    model.language_projection,\n",
    "    model.language_model,\n",
    "]\n",
    "\n",
    "freeze_list = [\n",
    "    # model.vision_model,\n",
    "    # model.qformer,\n",
    "    # model.language_projection,\n",
    "    model.language_model,\n",
    "]\n",
    "\n",
    "for freeze_block in freeze_list:\n",
    "    for name, param in freeze_block.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    freeze_block = freeze_block.eval()\n",
    "\n",
    "for block in block_list:\n",
    "    if block not in freeze_list:\n",
    "        for name, param in block.named_parameters():\n",
    "            param.requires_grad = True\n",
    "    block = block.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../training_outputs/{model_name}\",\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=epochs,\n",
    "    fp16=True if dtype is torch.float16 else False,\n",
    "    fp16_opt_level=\"02\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=1,\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=['labels'],\n",
    ")\n",
    "training_args.set_lr_scheduler(name='linear', warmup_steps=1000)\n",
    "training_args.set_optimizer(name='adamw_hf', learning_rate=1e-6, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "with autocast(dtype=dtype):\n",
    "    result = trainer.train()\n",
    "    print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from utils import show_image_caption\n",
    "\n",
    "image = '../datasets/cvpr-nice-val/val/215268662.jpg'\n",
    "# caption_gt = 'Bicycles leaning against tree in wood Close up low angle view'\n",
    "raw_image = Image.open(image).convert('RGB')\n",
    "\n",
    "inputs = processor(raw_image, return_tensors=\"pt\").to(start_device, dtype)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=max_length)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n",
    "\n",
    "show_image_caption(raw_image, [generated_text], show_fig=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_NICE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
