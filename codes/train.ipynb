{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from textwrap import wrap\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataset_config import COCO_dataset_config\n",
    "from train_configuration import DatasetType, TrainConfiguration\n",
    "from custom_datasets.coco_dataset import COCOTrainDataset, COCOEvalDataset\n",
    "from utils import show_image_caption\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "print('Use ', torch.cuda.get_device_name(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Config, Blip2VisionConfig, Blip2QFormerConfig, OPTConfig\n",
    "\n",
    "# configuration = Blip2Config()\n",
    "# vision_config = Blip2VisionConfig()\n",
    "# qformer_config = Blip2QFormerConfig()\n",
    "# text_config = OPTConfig()\n",
    "\n",
    "# config = Blip2Config.from_text_vision_config(vision_config, qformer_config, text_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blip2 설정 참고하기\n",
    "# https://github.com/salesforce/LAVIS/blob/main/lavis/projects/blip2/train/caption_coco_ft.yaml\n",
    "\n",
    "checkpoint = \"Salesforce/blip2-opt-2.7b\"\n",
    "dtype = torch.float16\n",
    "\n",
    "model_name = checkpoint.split(\"/\")[1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../training_outputs/{model_name}\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=50,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"texts\"],\n",
    "    load_best_model_at_end=True,\n",
    "    optim='adamw_hf',\n",
    "    lr_scheduler_type='linear', # github에서는 linear_warmup_cosine_lr <- 구현해놓은건지 확인 필요\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_type = DatasetType.COCO\n",
    "# epochs = 100\n",
    "# batch_size = 1\n",
    "# lr = 1e-4\n",
    "# embed_size = 256\n",
    "\n",
    "\n",
    "# train_configuration = TrainConfiguration(\n",
    "#     dataset_type=dataset_type,\n",
    "#     epochs=epochs,\n",
    "#     batch_size=batch_size,\n",
    "#     lr=lr,\n",
    "#     embed_size=embed_size,\n",
    "#     dtype=dtype,\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor\n",
    "##### image-processor + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\n",
    "    checkpoint,\n",
    "    cache_dir='../pretrained_files',\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = COCOTrainDataset(\n",
    "    img_dir=\"../datasets/COCO/images/train2017\",\n",
    "    ann_file=\"../datasets/COCO/annotations/captions_train2017.json\",\n",
    "    vis_processor=processor.image_processor,\n",
    "    text_processor=processor.tokenizer,\n",
    ")\n",
    "\n",
    "test_ds = COCOEvalDataset(\n",
    "    img_dir=\"../datasets/COCO/images/val2017\",\n",
    "    ann_file=\"../datasets/COCO/annotations/captions_val2017.json\",\n",
    "    vis_processor=processor.image_processor,\n",
    "    text_processor=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(normalized_image, mean, std):\n",
    "    image = normalized_image.transpose(1, 2, 0)\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def plot_images(images, captions):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images)):\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        caption = captions[i]\n",
    "        caption = \"\\n\".join(wrap(caption, 16))\n",
    "        plt.title(caption)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "num_samples = 5\n",
    "samples = [train_ds[i] for i in range(num_samples)]\n",
    "\n",
    "sample_images = []\n",
    "sample_captions = []\n",
    "for i in range(num_samples):\n",
    "    sample_image = np.array(samples[i]['image']['pixel_values'][0])\n",
    "    sample_image = denormalize_image(sample_image, processor.image_processor.image_mean, processor.image_processor.image_std)\n",
    "    sample_images.append(sample_image)\n",
    "    \n",
    "    sample_caption = ''.join(processor.batch_decode(samples[i]['text_input']['input_ids'], skip_special_tokens=True))\n",
    "    sample_captions.append(sample_caption)\n",
    "\n",
    "plot_images(sample_images, sample_captions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = {\n",
    "    \"query_tokens\": 0,\n",
    "    \"vision_model.embeddings\": 0,\n",
    "    \"vision_model.encoder.layers.0\": 0,\n",
    "    \"vision_model.encoder.layers.1\": 0,\n",
    "    \"vision_model.encoder.layers.2\": 0,\n",
    "    \"vision_model.encoder.layers.3\": 0,\n",
    "    \"vision_model.encoder.layers.4\": 0,\n",
    "    \"vision_model.encoder.layers.5\": 0,\n",
    "    \"vision_model.encoder.layers.6\": 0,\n",
    "    \"vision_model.encoder.layers.7\": 0,\n",
    "    \"vision_model.encoder.layers.8\": 0,\n",
    "    \"vision_model.encoder.layers.9\": 0,\n",
    "    \"vision_model.encoder.layers.10\": 1,\n",
    "    \"vision_model.encoder.layers.11\": 1,\n",
    "    \"vision_model.encoder.layers.12\": 1,\n",
    "    \"vision_model.encoder.layers.13\": 1,\n",
    "    \"vision_model.encoder.layers.14\": 1,\n",
    "    \"vision_model.encoder.layers.15\": 1,\n",
    "    \"vision_model.encoder.layers.16\": 1,\n",
    "    \"vision_model.encoder.layers.17\": 1,\n",
    "    \"vision_model.encoder.layers.18\": 1,\n",
    "    \"vision_model.encoder.layers.19\": 1,\n",
    "    \"vision_model.encoder.layers.20\": 1,\n",
    "    \"vision_model.encoder.layers.21\": 1,\n",
    "    \"vision_model.encoder.layers.22\": 1,\n",
    "    \"vision_model.encoder.layers.23\": 1,\n",
    "    \"vision_model.encoder.layers.24\": 1,\n",
    "    \"vision_model.encoder.layers.25\": 1,\n",
    "    \"vision_model.encoder.layers.26\": 1,\n",
    "    \"vision_model.encoder.layers.27\": 1,\n",
    "    \"vision_model.encoder.layers.28\": 1,\n",
    "    \"vision_model.encoder.layers.29\": 1,\n",
    "    \"vision_model.encoder.layers.30\": 1,\n",
    "    \"vision_model.encoder.layers.31\": 1,\n",
    "    \"vision_model.encoder.layers.32\": 1,\n",
    "    \"vision_model.encoder.layers.33\": 1,\n",
    "    \"vision_model.encoder.layers.34\": 1,\n",
    "    \"vision_model.encoder.layers.35\": 1,\n",
    "    \"vision_model.encoder.layers.36\": 1,\n",
    "    \"vision_model.encoder.layers.38\": 1,\n",
    "    \"vision_model.post_layernorm\": 1,\n",
    "    \"qformer\": 1,\n",
    "    \"language_projection\": 1,\n",
    "    \"language_model.model.decoder.embed_tokens\": 1,\n",
    "    \"language_model.lm_head\": 2,\n",
    "    \"language_model.model.decoder.embed_positions\": 2,\n",
    "    \"language_model.model.decoder.final_layer_norm\": 2,\n",
    "    \"language_model.model.decoder.layers.0\": 2,\n",
    "    \"language_model.model.decoder.layers.1\": 2,\n",
    "    \"language_model.model.decoder.layers.2\": 2,\n",
    "    \"language_model.model.decoder.layers.3\": 2,\n",
    "    \"language_model.model.decoder.layers.4\": 2,\n",
    "    \"language_model.model.decoder.layers.5\": 2,\n",
    "    \"language_model.model.decoder.layers.6\": 2,\n",
    "    \"language_model.model.decoder.layers.7\": 2,\n",
    "    \"language_model.model.decoder.layers.8\": 2,\n",
    "    \"language_model.model.decoder.layers.9\": 2,\n",
    "    \"language_model.model.decoder.layers.10\": 2,\n",
    "    \"language_model.model.decoder.layers.11\": 2,\n",
    "    \"language_model.model.decoder.layers.12\": 2,\n",
    "    \"language_model.model.decoder.layers.13\": 2,\n",
    "    \"language_model.model.decoder.layers.14\": 3,\n",
    "    \"language_model.model.decoder.layers.15\": 3,\n",
    "    \"language_model.model.decoder.layers.16\": 3,\n",
    "    \"language_model.model.decoder.layers.17\": 3,\n",
    "    \"language_model.model.decoder.layers.18\": 3,\n",
    "    \"language_model.model.decoder.layers.19\": 3,\n",
    "    \"language_model.model.decoder.layers.20\": 3,\n",
    "    \"language_model.model.decoder.layers.21\": 3,\n",
    "    \"language_model.model.decoder.layers.22\": 3,\n",
    "    \"language_model.model.decoder.layers.23\": 3,\n",
    "    \"language_model.model.decoder.layers.24\": 3,\n",
    "    \"language_model.model.decoder.layers.25\": 3,\n",
    "    \"language_model.model.decoder.layers.26\": 3,\n",
    "    \"language_model.model.decoder.layers.27\": 3,\n",
    "    \"language_model.model.decoder.layers.28\": 3,\n",
    "    \"language_model.model.decoder.layers.29\": 3,\n",
    "    \"language_model.model.decoder.layers.30\": 3,\n",
    "    \"language_model.model.decoder.layers.31\": 3,\n",
    "    \"vision_model.encoder.layers.37\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2ForConditionalGeneration\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    checkpoint,\n",
    "    cache_dir='../pretrained_files',\n",
    "    torch_dtype=dtype,\n",
    "    device_map=device_map\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    # loss = pred[\"loss\"]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/salesforce/LAVIS/blob/5ee63d688ba4cebff63acee04adaef2dee9af207/lavis/models/blip2_models/blip2_opt.py\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, dataloader):\n",
    "#     logging.info(\"Start Training...\")\n",
    "#     for image, caption in tqdm(dataloader):\n",
    "#         print(caption)\n",
    "#         # plt.imshow(image[0])\n",
    "#         # print(caption[0])\n",
    "        \n",
    "        \n",
    "#         loss = model.opt_model()\n",
    "#         # loss = model._modules['vision_model']\n",
    "#         show_image_caption(image, caption, save_path='./sample.png')\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_NICE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
